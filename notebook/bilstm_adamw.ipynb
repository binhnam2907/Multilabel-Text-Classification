{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import các thư viện cần thiết**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "import unidecode\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "RANDOM_SEED = 1\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Tiền xử lý**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = stopwords.words('english') # Lấy danh sách stopwords từ thư viện ntlk\n",
    "stemmer = PorterStemmer() # Khai báo stemmer object (dùng để stemming trong hàm normalize text)\n",
    "\n",
    "# Xây dựng hàm text normalization\n",
    "def text_normalize(text):\n",
    "    text = text.lower() # Chuyển chữ viết thường \n",
    "    text = unidecode.unidecode(text) # Mã hóa về ASCII\n",
    "    text = text.strip() # Xóa kí tự đặc biệt ở đầu và cuối string\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Loại bỏ dấu câu\n",
    "    text = ' '.join([word for word in text.split(' ') if word not in english_stop_words]) # Xóa stopwords\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split(' ')]) # Stemming\n",
    " \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "MAX_SEQ_LEN = 128\n",
    "MAX_FEATURES = 5000 \n",
    "EMBEDDING_DIMS = 64\n",
    "\n",
    "train_filepath = \"dataset/train.csv\"\n",
    "val_filepath = \"dataset/val.csv\"\n",
    "test_filepath = \"dataset/test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_filepath, \n",
    "                index_col=0) \n",
    "val_df = pd.read_csv(val_filepath, \n",
    "                index_col=0) \n",
    "test_df = pd.read_csv(test_filepath, \n",
    "                index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Tweet'] = train_df['Tweet'].apply(lambda p: text_normalize(p)).astype(str) \n",
    "val_df['Tweet'] = val_df['Tweet'].apply(lambda p: text_normalize(p)).astype(str) \n",
    "test_df['Tweet'] = test_df['Tweet'].apply(lambda p: text_normalize(p)).astype(str) \n",
    "\n",
    "class_lst = np.array(train_df.columns[2:])\n",
    "n_classes = len(class_lst)\n",
    "\n",
    "X_train, y_train = train_df['Tweet'].to_numpy(), train_df[class_lst].astype('int').to_numpy()\n",
    "X_val, y_val = val_df['Tweet'].to_numpy(), val_df[class_lst].astype('int').to_numpy()\n",
    "X_test, y_test = test_df['Tweet'].to_numpy(), test_df[class_lst].astype('int').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pessimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-En-21441</td>\n",
       "      <td>worri payment problem may never  joyc meyer  m...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-En-31535</td>\n",
       "      <td>whatev decid make sure make happi</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-En-21068</td>\n",
       "      <td>max_kellerman  also help major nfl coach inept...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                              Tweet  anger  \\\n",
       "0  2017-En-21441  worri payment problem may never  joyc meyer  m...  False   \n",
       "1  2017-En-31535                  whatev decid make sure make happi  False   \n",
       "2  2017-En-21068  max_kellerman  also help major nfl coach inept...   True   \n",
       "\n",
       "   anticipation  disgust   fear    joy   love  optimism  pessimism  sadness  \\\n",
       "0          True    False  False  False  False      True      False    False   \n",
       "1         False    False  False   True   True      True      False    False   \n",
       "2         False     True  False   True  False      True      False    False   \n",
       "\n",
       "   surprise  trust  \n",
       "0     False   True  \n",
       "1     False  False  \n",
       "2     False  False  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet 0:  b'worri payment problem may never  joyc meyer  motiv leadership worri'\n",
      "Tweet 1:  b'whatev decid make sure make happi'\n",
      "Tweet 2:  b'max_kellerman  also help major nfl coach inept bill obrien play call wow  gopat'\n",
      "Tweet 3:  b'accept challeng liter even feel exhilar victori  georg patton'\n",
      "Tweet 4:  b'roommat okay cant spell autocorrect terribl firstworldprob'\n",
      "Tweet 5:  b'that cute atsu probabl shi photo cherri help uwu'\n",
      "Tweet 6:  b'think human sens recogn impend doom'\n",
      "Tweet 7:  b'rooney fuck untouch isnt fuck dread depay look decentishtonight'\n",
      "Tweet 8:  b'pretti depress u hit pan ur favourit highlight'\n",
      "Tweet 9:  b'bossupjae pussi weak heard stfu bitch  got threaten pregnant '\n"
     ]
    }
   ],
   "source": [
    "def inverse_label(class_lst, onehot_label):\n",
    "\n",
    "    return class_lst[onehot_label > 0]\n",
    "\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(10):\n",
    "    print(f\"Tweet {i}: \", text_batch.numpy()[i])\n",
    "    print(\"Label:\", inverse_label(class_lst, label_batch.numpy()[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cấu hình các tham số tối ưu cho việc đọc dữ liệu\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Xây dựng mô hình**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo layer text vectorization\n",
    "text_vectorization_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_FEATURES, # Kích thước bộ từ vựng\n",
    "    output_mode='int', # Giá trị token là chỉ mục của từ trong vocab\n",
    "    output_sequence_length=MAX_SEQ_LEN # Số token tối đa trong 1 vector\n",
    ")\n",
    "\n",
    "train_text = train_ds.map(lambda text, labels: text) # Gọi `content` của toàn bộ mẫu dữ liệu trong tập train\n",
    "text_vectorization_layer.adapt(train_text) # Xây dựng layer vectorization dựa trên dữ liệu tập train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng hàm khởi tạo model\n",
    "def build_model(max_features, max_seq_len, embedding_dims, n_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Input \n",
    "        tf.keras.Input(shape=(1,), dtype=\"string\", name=\"input_shape\"),\n",
    "\n",
    "        #text vectorization\n",
    "        text_vectorization_layer,\n",
    "\n",
    "        #Embedding layer\n",
    "        tf.keras.layers.Embedding(input_dim=max_features+1,\n",
    "                                  output_dim=embedding_dims,\n",
    "                                  embeddings_initializer=tf.random_uniform_initializer,\n",
    "                                  mask_zero=True,\n",
    "                                  name='embedding_layer'),\n",
    "        # Bi-LSTM Layer 1 \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, \n",
    "                                    return_sequences=True, \n",
    "                                    kernel_initializer=tf.initializers.GlorotUniform(seed=RANDOM_SEED)),\n",
    "                                    name='bilstm_layer_1'),\n",
    "                                 \n",
    "        # Bi-LSTM Layer 2\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, \n",
    "                                    return_sequences=True, \n",
    "                                    kernel_initializer=tf.initializers.GlorotUniform(seed=RANDOM_SEED)),\n",
    "                                    name='bilstm_layer_2'),\n",
    "                                 \n",
    "        # Bi-LSTM Layer 3\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, \n",
    "                                    return_sequences=False, \n",
    "                                    kernel_initializer=tf.initializers.GlorotUniform(seed=RANDOM_SEED)),\n",
    "                                    name='bilstm_layer_3'),\n",
    "        #Dropout\n",
    "        tf.keras.layers.Dropout(0.2, name='dropout_layer'),\n",
    "\n",
    "        # Fully-connected Layer 2\n",
    "        tf.keras.layers.Dense(32, \n",
    "                              activation='relu',\n",
    "                              kernel_initializer=tf.initializers.GlorotUniform(seed=RANDOM_SEED),\n",
    "                              name='fl_layer_2'),\n",
    "\n",
    "        # Dropout Layer 2\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "        # Output Layer\n",
    "        tf.keras.layers.Dense(n_classes, \n",
    "                              activation='sigmoid', \n",
    "                              kernel_initializer=tf.initializers.GlorotUniform(seed=RANDOM_SEED),\n",
    "                              name='output_layer') \n",
    "\n",
    "        \n",
    "    ],\n",
    "    name='bilstm_model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bilstm_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (TextV  (None, 128)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_layer (Embedding)  (None, 128, 64)          320064    \n",
      "                                                                 \n",
      " bilstm_layer_1 (Bidirection  (None, 128, 128)         66048     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bilstm_layer_2 (Bidirection  (None, 128, 128)         98816     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bilstm_layer_3 (Bidirection  (None, 128)              98816     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout_layer (Dropout)     (None, 128)               0         \n",
      "                                                                 \n",
      " fl_layer_2 (Dense)          (None, 32)                4128      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 11)                363       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 588,235\n",
      "Trainable params: 588,235\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(max_features=MAX_FEATURES, \n",
    "                    max_seq_len=MAX_SEQ_LEN,\n",
    "                    embedding_dims=EMBEDDING_DIMS, \n",
    "                    n_classes=n_classes)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Cấu hình mô hình**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo một số giá trị siêu tham số\n",
    "EPOCHS = 30\n",
    "\n",
    "step = tf.Variable(0, trainable=False)\n",
    "schedule = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    [10000, 15000], [1e-1, 1e-2, 1e-3])\n",
    "\n",
    "lr = 1e-2 * schedule(step)\n",
    "wd = lambda: 1e-5 * schedule(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cấu hình một số thông tin cho mô hình\n",
    "model.compile(\n",
    "    optimizer=tfa.optimizers.AdamW(weight_decay=wd,\n",
    "                                   learning_rate=lr), # Sử dụng optimizer AdamW\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(), # Sử dụng hàm loss BinaryCrossEntropy\n",
    "    metrics=['accuracy'] # Sử dụng thêm độ đo đánh giá Accuracy\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Thực hiện huấn luyện**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "54/54 [==============================] - 288s 4s/step - loss: 0.5858 - accuracy: 0.1003 - val_loss: 0.5049 - val_accuracy: 0.0632\n",
      "Epoch 2/30\n",
      "54/54 [==============================] - 304s 6s/step - loss: 0.4901 - accuracy: 0.2384 - val_loss: 0.4302 - val_accuracy: 0.3047\n",
      "Epoch 3/30\n",
      "54/54 [==============================] - 303s 6s/step - loss: 0.4154 - accuracy: 0.3533 - val_loss: 0.4039 - val_accuracy: 0.3679\n",
      "Epoch 4/30\n",
      " 3/54 [>.............................] - ETA: 5:31 - loss: 0.3943 - accuracy: 0.3646"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\project\\Multilabel-Text-Classification\\notebook\\bilstm_adamw.ipynb Cell 19\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/project/Multilabel-Text-Classification/notebook/bilstm_adamw.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Thực hiện huấn luyện\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/project/Multilabel-Text-Classification/notebook/bilstm_adamw.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit( \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/project/Multilabel-Text-Classification/notebook/bilstm_adamw.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_ds, \u001b[39m# Huấn luyện với bộ train_ds\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/project/Multilabel-Text-Classification/notebook/bilstm_adamw.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_ds, \u001b[39m# Đánh giá trên bộ val_ds\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/project/Multilabel-Text-Classification/notebook/bilstm_adamw.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mEPOCHS \u001b[39m# Huấn luyện với số lần lặp = số epochs\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/project/Multilabel-Text-Classification/notebook/bilstm_adamw.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Thực hiện huấn luyện\n",
    "history = model.fit( \n",
    "    train_ds, # Huấn luyện với bộ train_ds\n",
    "    validation_data=val_ds, # Đánh giá trên bộ val_ds\n",
    "    epochs=EPOCHS # Huấn luyện với số lần lặp = số epochs\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Đánh giá và trực quan hóa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đánh giá mô hình trên tập test\n",
    "test_evaluation = model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc các kết quả huấn luyện mô hình qua từng epoch\n",
    "train_loss, train_acc = history.history['loss'], history.history['accuracy'] # Đọc thông tin loss, acc trên tập train\n",
    "val_loss, val_acc = history.history['val_loss'], history.history['val_accuracy'] # Đọc thông tin loss, acc trên tập val\n",
    "\n",
    "plt.figure(figsize=(10, 10)) # Cài đặt kích thước khung ảnh\n",
    "\n",
    "plt.subplot(2, 2, 1) # Khởi tạo khung ảnh cho training loss\n",
    "plt.xlabel('Epochs') # Hiển thị tên trục hoành là 'Epochs'\n",
    "plt.ylabel('Loss') # Hiển thị tên trục tung là 'Loss'\n",
    "plt.title('Training loss') # Hiển thị title của khung ảnh hiện tại là 'Training Loss'\n",
    "plt.plot(train_loss, color='red') # Vẽ đường giá trị loss trên tập train qua từng epoch (đường vẽ màu đỏ)\n",
    "\n",
    "plt.subplot(2, 2, 2) # Khởi tạo khung ảnh cho training acc\n",
    "plt.xlabel('Epochs') # Hiển thị tên trục hoành là 'Epochs'\n",
    "plt.ylabel('Accuracy') # Hiển thị tên trục tung là 'Accuracy'\n",
    "plt.title('Training accuracy') # Hiển thị title của khung ảnh hiện tại là 'Training accuracy'\n",
    "plt.plot(train_acc, color='orange') # Vẽ đường giá trị accuracy trên tập train qua từng epoch (đường vẽ màu cam)\n",
    "\n",
    "plt.subplot(2, 2, 3) # Khởi tạo khung ảnh cho val loss\n",
    "plt.xlabel('Epochs') # Hiển thị tên trục hoành là 'Epochs'\n",
    "plt.ylabel('Loss') # Hiển thị tên trục tung là 'Loss'\n",
    "plt.title('Validation loss') # Hiển thị title của khung ảnh hiện tại là 'Validation loss'\n",
    "plt.plot(val_loss, color='red') # Vẽ đường giá trị loss trên tập val qua từng epoch (đường vẽ màu đỏ)\n",
    "\n",
    "plt.subplot(2, 2, 4) # Khởi tạo khung ảnh cho val acc\n",
    "plt.xlabel('Epochs') # Hiển thị tên trục hoành là 'Epochs'\n",
    "plt.ylabel('Accuracy') # Hiển thị tên trục tung là 'Accuracy'\n",
    "plt.title('Validation accuracy') # Hiển thị title của khung ảnh hiện tại là 'Validation accuracy'\n",
    "plt.plot(val_acc, color='orange') # Vẽ đường giá trị accuracy trên tập val qua từng epoch (đường vẽ màu cam)\n",
    "\n",
    "plt.show() # Hiển thị 4 khung ảnh nhỏ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.4\n",
    "\n",
    "for text_batch, label_batch in test_ds.take(1):\n",
    "    for i in range(10):\n",
    "        input_text = text_batch[i].numpy()\n",
    "        label = label_batch[i].numpy()\n",
    "        pred = model.predict(np.expand_dims(input_text, 0), verbose=0)[0]\n",
    "        threshold_pred = np.where(pred > threshold, 1, 0)\n",
    "        print(f\"Text: {input_text}\")\n",
    "        print(f\"Label: {inverse_label(class_lst, label)}\")\n",
    "        print(f\"Predicted Label(s): ({inverse_label(class_lst, threshold_pred)})\")\n",
    "        print(\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
