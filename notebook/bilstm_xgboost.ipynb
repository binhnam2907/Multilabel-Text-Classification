{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import các thư viện cần thiết**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "import unidecode\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Tiền xử lý**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = stopwords.words('english') # Lấy danh sách stopwords từ thư viện ntlk\n",
    "stemmer = PorterStemmer() # Khai báo stemmer object (dùng để stemming trong hàm normalize text)\n",
    "\n",
    "# Xây dựng hàm text normalization\n",
    "def text_normalize(text):\n",
    "    text = text.lower() # Chuyển chữ viết thường \n",
    "    text = unidecode.unidecode(text) # Mã hóa về ASCII\n",
    "    text = text.strip() # Xóa kí tự đặc biệt ở đầu và cuối string\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Loại bỏ dấu câu\n",
    "    text = ' '.join([word for word in text.split(' ') if word not in english_stop_words]) # Xóa stopwords\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split(' ')]) # Stemming\n",
    " \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LR = 1e-1\n",
    "MAX_SEQ_LEN = 128\n",
    "MAX_FEATURES = 5000 \n",
    "EMBEDDING_DIMS = 64\n",
    "\n",
    "train_filepath = \"dataset/train.csv\"\n",
    "val_filepath = \"dataset/val.csv\"\n",
    "test_filepath = \"dataset/test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_filepath, \n",
    "                index_col=0) \n",
    "val_df = pd.read_csv(val_filepath, \n",
    "                index_col=0) \n",
    "test_df = pd.read_csv(test_filepath, \n",
    "                index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Tweet'] = train_df['Tweet'].apply(lambda p: text_normalize(p)).astype(str) \n",
    "val_df['Tweet'] = val_df['Tweet'].apply(lambda p: text_normalize(p)).astype(str) \n",
    "test_df['Tweet'] = test_df['Tweet'].apply(lambda p: text_normalize(p)).astype(str) \n",
    "\n",
    "class_lst = np.array(train_df.columns[2:])\n",
    "n_classes = len(class_lst)\n",
    "\n",
    "X_train, y_train = train_df['Tweet'].to_numpy(), train_df[class_lst].astype('int').to_numpy()\n",
    "X_val, y_val = val_df['Tweet'].to_numpy(), val_df[class_lst].astype('int').to_numpy()\n",
    "X_test, y_test = test_df['Tweet'].to_numpy(), test_df[class_lst].astype('int').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = X_train.tolist()\n",
    "vectorizer = TfidfVectorizer().fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(X_train)\n",
    "X_val = vectorizer.transform(X_val)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_label(class_lst, onehot_label):\n",
    "\n",
    "    return class_lst[onehot_label > 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Xây dựng mô hình**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = XGBClassifier(objective='binary:logistic',\n",
    "                              learning_rate=LR,\n",
    "                              verbosity=2)\n",
    "xgboost_multilabel_model = MultiOutputClassifier(xgboost_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Huấn luyện và đánh giá**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_multilabel_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_multilabel_model.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
